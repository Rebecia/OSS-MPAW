{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # embedding\n",
    "\n",
    "# set up the model\n",
    "NAME = ' '\n",
    "\n",
    "def load_malicious_code(base_path):\n",
    "    \"\"\"load malicious code from the specified directory\"\"\"\n",
    "    malicious_codes = []\n",
    "    file_paths = []\n",
    "    \n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.startswith('._') or not file.endswith('.py'):\n",
    "                continue\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                for encoding in ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding=encoding) as f:\n",
    "                            code = f.read()\n",
    "                            if code.strip() and isinstance(code, str):\n",
    "                                malicious_codes.append(code)\n",
    "                                file_paths.append(file_path)\n",
    "                                break\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(malicious_codes)} files\")\n",
    "    return malicious_codes, file_paths\n",
    "\n",
    "def split_code_into_segments(code, max_length=512):\n",
    "    \"\"\"break your code into smaller pieces to avoid long inputs\"\"\"\n",
    "    lines = code.split('\\n')\n",
    "    segments = []\n",
    "    current_segment = []\n",
    "\n",
    "    for line in lines:\n",
    "        current_segment.append(line)\n",
    "        if len(' '.join(current_segment).split()) > max_length:\n",
    "            segments.append('\\n'.join(current_segment[:-1])) \n",
    "            current_segment = [current_segment[-1]] \n",
    "\n",
    "    if current_segment:\n",
    "        segments.append('\\n'.join(current_segment))\n",
    "\n",
    "    return segments\n",
    "\n",
    "def attention_based_encoding(code, tokenizer, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"use code bert to encode the code\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(code, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding: {e}\")\n",
    "        return None\n",
    "\n",
    "def attention_based_encoding_segmented(code, tokenizer, model, device='cuda'):\n",
    "    \"\"\"each code snippet is encoded and the encoding of each snippet is returned\"\"\"\n",
    "    segments = split_code_into_segments(code)\n",
    "    encoded_segments = []\n",
    "\n",
    "    for segment in segments:\n",
    "        encoding = attention_based_encoding(segment, tokenizer, model, device)\n",
    "        if encoding is not None:\n",
    "            encoded_segments.append(encoding)\n",
    "\n",
    "    return np.mean(encoded_segments, axis=0) if encoded_segments else None\n",
    "\n",
    "def save_embeddings_to_json(base_path, output_path):\n",
    "    \"\"\"calculate and save the code embedding\"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(NAME)\n",
    "    model = AutoModel.from_pretrained(NAME).to(device)\n",
    "\n",
    "    print(\"Loading malicious code...\")\n",
    "    codes, file_paths = load_malicious_code(base_path)\n",
    "\n",
    "    print(\"Encoding code samples...\")\n",
    "    encodings = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i in tqdm(range(len(codes))):\n",
    "        code = codes[i]\n",
    "        encoding = attention_based_encoding_segmented(code, tokenizer, model, device)\n",
    "        if encoding is not None:\n",
    "            encodings.append(encoding)\n",
    "            valid_indices.append(i)\n",
    "\n",
    "    X = np.vstack(encodings)\n",
    "\n",
    "    # save the embedding vector to a json file\n",
    "    embedding_output = {\n",
    "        \"file_paths\": file_paths,\n",
    "        \"embeddings\": X.tolist(),\n",
    "    }\n",
    "\n",
    "    output_file = Path(output_path) / \"malware_embeddings_demo.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(embedding_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Embeddings saved to {output_file}\")\n",
    "\n",
    "base_path = \" \"\n",
    "output_path = \" \"\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "save_embeddings_to_json(base_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_from_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    embeddings = []\n",
    "    file_paths = []\n",
    "    source_codes = []\n",
    "    \n",
    "    for item in data:  # No More Data Get Data\n",
    "        embeddings.append(item[\"embedding\"])  \n",
    "        file_paths.append(item[\"file_path\"])  \n",
    "        source_codes.append(item[\"source_code\"])  \n",
    "    \n",
    "    return np.array(embeddings), file_paths, source_codes\n",
    "\n",
    "def calculate_cluster_similarity(X_scaled, cluster_labels, cluster_id):\n",
    "    \"\"\"the mean cosine similarity of the samples within the cluster was calculated\"\"\"\n",
    "    cluster_samples = X_scaled[cluster_labels == cluster_id]\n",
    "    if len(cluster_samples) < 2:\n",
    "        return 1.0\n",
    "    similarities = cosine_similarity(cluster_samples)\n",
    "    upper_tri = similarities[np.triu_indices(len(similarities), k=1)]\n",
    "    return np.mean(upper_tri)\n",
    "\n",
    "\n",
    "\n",
    "def numpy_default(o):\n",
    "    if isinstance(o, np.int64):\n",
    "        return int(o)  \n",
    "    if isinstance(o, np.float64):\n",
    "        return float(o)  \n",
    "    if isinstance(o, np.ndarray):  \n",
    "        return o.tolist()\n",
    "    raise TypeError(f\"Object of type {o.__class__.__name__} is not JSON serializable\")\n",
    "\n",
    "def save_cluster_results(cluster_summary, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Use The Default Parameter To Specify A Custom Serialization Method\n",
    "        json.dump(cluster_summary, f, indent=2, ensure_ascii=False, default=numpy_default)\n",
    "\n",
    "def perform_clustering(X_scaled, file_paths, source_codes, n_clusters=500, min_similarity=0.7, max_cluster_size=100):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20, max_iter=500)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    sample_silhouette_values = silhouette_samples(X_scaled, clusters)\n",
    "    \n",
    "    cluster_summary = {}\n",
    "    valid_cluster_count = 0\n",
    "    total_clustered_nodes = 0\n",
    "    total_similarity = 0  # accumulate the similarity of all clusters\n",
    "    valid_cluster_similarity_count = 0  # the number of valid clusters\n",
    "    total_cluster_size = 0  # used to accumulate the size of all valid clusters\n",
    "    \n",
    "    for cluster_id in tqdm(range(n_clusters), desc=\"Processing Clusters\", unit=\"cluster\"):\n",
    "        cluster_indices = np.where(clusters == cluster_id)[0]\n",
    "        \n",
    "        if len(cluster_indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        cluster_similarity = calculate_cluster_similarity(X_scaled, clusters, cluster_id)\n",
    "        \n",
    "        if cluster_similarity < min_similarity:\n",
    "            continue\n",
    "        \n",
    "        cluster_members = []\n",
    "        cluster_silhouette_values = []\n",
    "        \n",
    "        for idx in cluster_indices:\n",
    "            member = {\n",
    "                \"source_code\": source_codes[idx],  \n",
    "                \"file_path\": file_paths[idx],  \n",
    "                \"distance_to_center\": float(np.linalg.norm(X_scaled[idx] - kmeans.cluster_centers_[cluster_id])),\n",
    "                \"silhouette_score\": float(sample_silhouette_values[idx]),\n",
    "                # \"embedding\": X_scaled[idx]  \n",
    "            }\n",
    "            cluster_members.append(member)\n",
    "            cluster_silhouette_values.append(sample_silhouette_values[idx])\n",
    "        \n",
    "        # Sort The Clusters And Filter The Samples With Low Profile Coefficients\n",
    "        cluster_members.sort(key=lambda x: (-x[\"silhouette_score\"], x[\"distance_to_center\"]))\n",
    "        filtered_members = [m for m in cluster_members if m[\"silhouette_score\"] > 0.3]\n",
    "        \n",
    "        if len(filtered_members) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Update The Statistics For The Cluster\n",
    "        cluster_summary[str(valid_cluster_count)] = {\n",
    "            \"size\": len(filtered_members),\n",
    "            \"average_similarity\": float(cluster_similarity),\n",
    "            \"average_silhouette\": float(np.mean(cluster_silhouette_values)),\n",
    "            \"samples\": filtered_members\n",
    "        }\n",
    "        \n",
    "        # Cumulative Similarity And Number Of Valid Clusters\n",
    "        total_similarity += cluster_similarity\n",
    "        valid_cluster_similarity_count += 1\n",
    "        \n",
    "        total_clustered_nodes += len(filtered_members)\n",
    "        total_cluster_size += len(filtered_members)  \n",
    "        valid_cluster_count += 1\n",
    "\n",
    "    # Calculate The Average Similarity Of All Valid Clusters\n",
    "    if valid_cluster_similarity_count > 0:\n",
    "        average_similarity_all_clusters = total_similarity / valid_cluster_similarity_count\n",
    "    else:\n",
    "        average_similarity_all_clusters = 0\n",
    "\n",
    "    # Calculate The Average Number Of Packets Per Active Cluster\n",
    "    average_cluster_size = total_cluster_size / valid_cluster_count if valid_cluster_count > 0 else 0\n",
    "\n",
    "    return cluster_summary, total_clustered_nodes, average_similarity_all_clusters, valid_cluster_count, average_cluster_size\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    embedding_file_path = \" \"\n",
    "    X_scaled, file_paths, source_codes = load_embeddings_from_json(embedding_file_path)\n",
    "\n",
    "    cluster_summary, total_clustered_nodes, average_similarity_all_clusters, total_clusters, average_cluster_size = perform_clustering(X_scaled, file_paths, source_codes, n_clusters=500)\n",
    "\n",
    "    print(f\"Total number of nodes clustered: {total_clustered_nodes}\")\n",
    "    print(f\"Average similarity of all clusters: {average_similarity_all_clusters:.4f}\")\n",
    "    print(f\"Total number of valid clusters: {total_clusters}\")\n",
    "    print(f\"Average number of packages per cluster: {average_cluster_size:.4f}\")\n",
    "\n",
    "    output_file = \" \"\n",
    "    save_cluster_results(cluster_summary, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NPM\n",
    "# \n",
    "# Total number of nodes clustered: 2994\n",
    "# Average similarity of all clusters: 0.9999\n",
    "# Total number of valid clusters: 157\n",
    "# Average number of packages per cluster: 19.0701\n",
    "\n",
    "# PyPI\n",
    "# \n",
    "# Total number of nodes clustered: 4365\n",
    "# Average similarity of all clusters: 0.9993\n",
    "# Total number of valid clusters: 295\n",
    "# Average number of packages per cluster: 14.7966\n",
    "\n",
    "# Ruby\n",
    "# \n",
    "# Total number of nodes clustered: 83\n",
    "# Average similarity of all clusters: 0.9994\n",
    "# Total number of valid clusters: 37\n",
    "# Average number of packages per cluster: 2.2432"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
