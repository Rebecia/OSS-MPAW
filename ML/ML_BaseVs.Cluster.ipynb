{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# ==== path ====\n",
    "CLUSTERED_PATH = \"malware_clusters_results_js.json\"\n",
    "UNCLUSTERED_PATH = \"mal_embeddings_js_480.json\"\n",
    "LEGIT_PATH = \"legiti_embeddings_js.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_clustered(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    result = {}\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            result.update({cid: [s[\"embedding\"] for s in details[\"samples\"]] for cid, details in item.items()})\n",
    "    else:\n",
    "        result = {cid: [s[\"embedding\"] for s in details[\"samples\"]] for cid, details in data.items()}\n",
    "    return result\n",
    "\n",
    "def load_embeddings_unclustered(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)[\"embeddings\"]\n",
    "\n",
    "def load_embeddings_legit(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multimodel_cluster_vs_random(clusters, legit_all, rounds=10):\n",
    "    # drop singleton\n",
    "    clusters = {cid: embs for cid, embs in clusters.items() if len(embs) > 1}\n",
    "    cluster_ids = list(clusters.keys())\n",
    "\n",
    "    # machine pools\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "        \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42),\n",
    "        \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42)\n",
    "    }\n",
    "\n",
    "    # results\n",
    "    results_cluster = {model: [] for model in models}\n",
    "    results_random = {model: [] for model in models}\n",
    "\n",
    "    for round_idx in range(rounds):\n",
    "        random.seed(round_idx)\n",
    "        cluster_train, cluster_test = [], []\n",
    "        train_pool_mal = []\n",
    "\n",
    "        # Build training pool & test set (take 1 test per cluster, and the rest are used for cluster training or random pool)\n",
    "        for cid in cluster_ids:\n",
    "            embs = clusters[cid]\n",
    "            test_sample = random.sample(embs, 1)\n",
    "            remaining = [e for e in embs if e not in test_sample]\n",
    "            if not remaining:\n",
    "                continue\n",
    "            elif len(remaining) >= 2:\n",
    "                train_samples = random.sample(remaining, 2)\n",
    "            else:\n",
    "                train_samples = random.choices(remaining, k=2)\n",
    "            cluster_test.extend(test_sample)\n",
    "            cluster_train.extend(train_samples)\n",
    "            train_pool_mal.extend(remaining)\n",
    "\n",
    "        total_cluster_train = len(cluster_train)\n",
    "        legit_train = random.choices(legit_all, k=total_cluster_train)\n",
    "        legit_test = random.sample(legit_all, k=len(cluster_test))\n",
    "\n",
    "        Xc_train = np.array(cluster_train + legit_train)\n",
    "        yc_train = np.array([1]*len(cluster_train) + [0]*len(legit_train))\n",
    "        Xc_test = np.array(cluster_test + legit_test)\n",
    "        yc_test = np.array([1]*len(cluster_test) + [0]*len(legit_test))\n",
    "\n",
    "        Xr_train_mal = random.sample(train_pool_mal, total_cluster_train)\n",
    "        Xr_train = np.array(Xr_train_mal + legit_train)\n",
    "        yr_train = np.array([1]*len(Xr_train_mal) + [0]*len(legit_train))\n",
    "\n",
    "        #  Train and evaluate all models one by one\n",
    "        for name, model in models.items():\n",
    "            # Cluster-Based\n",
    "            clf_c = model.__class__(**model.get_params())\n",
    "            clf_c.fit(Xc_train, yc_train)\n",
    "            pred_c = clf_c.predict(Xc_test)\n",
    "            prob_c = clf_c.predict_proba(Xc_test)[:, 1]\n",
    "            results_cluster[name].append({\n",
    "                \"accuracy\": accuracy_score(yc_test, pred_c),\n",
    "                \"precision\": precision_score(yc_test, pred_c),\n",
    "                \"recall\": recall_score(yc_test, pred_c),\n",
    "                \"f1\": f1_score(yc_test, pred_c),\n",
    "                \"auc\": roc_auc_score(yc_test, prob_c)\n",
    "            })\n",
    "\n",
    "            # Random-Based\n",
    "            clf_r = model.__class__(**model.get_params())\n",
    "            clf_r.fit(Xr_train, yr_train)\n",
    "            pred_r = clf_r.predict(Xc_test)\n",
    "            prob_r = clf_r.predict_proba(Xc_test)[:, 1]\n",
    "            results_random[name].append({\n",
    "                \"accuracy\": accuracy_score(yc_test, pred_r),\n",
    "                \"precision\": precision_score(yc_test, pred_r),\n",
    "                \"recall\": recall_score(yc_test, pred_r),\n",
    "                \"f1\": f1_score(yc_test, pred_r),\n",
    "                \"auc\": roc_auc_score(yc_test, prob_r)\n",
    "            })\n",
    "\n",
    "    return results_cluster, results_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = load_embeddings_clustered(CLUSTERED_PATH)\n",
    "legit_all = load_embeddings_legit(LEGIT_PATH)\n",
    "results_cluster, results_random = run_multimodel_cluster_vs_random(clusters, legit_all, rounds=50)\n",
    "\n",
    "# print output\n",
    "for model in results_cluster:\n",
    "    df_c = pd.DataFrame(results_cluster[model])\n",
    "    df_r = pd.DataFrame(results_random[model])\n",
    "    print(f\"\\nüîç Model: {model}\")\n",
    "    print(\"üìä Cluster AVE.: \\n\", df_c.mean())\n",
    "    print(\"üìä Random  AVE.: \\n\", df_r.mean())\n",
    "    print(\"üî¨ Significance Test:\")\n",
    "    for metric in df_c.columns:\n",
    "        t, p = ttest_rel(df_c[metric], df_r[metric])\n",
    "        print(f\"  {metric:<10} p={p:.5f} {'‚úÖ Significant' if p < 0.05 else '‚ö†Ô∏è not Significant'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
