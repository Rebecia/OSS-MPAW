{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import os\n",
    "import openpyxl\n",
    "import random\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spilt test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legitimate\n",
    "def load_legit_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    data_type = None\n",
    "    if isinstance(raw_data, list):\n",
    "        if all(isinstance(x, list) for x in raw_data):  \n",
    "            data_type = \"list_of_lists\"\n",
    "        elif all(isinstance(x, dict) for x in raw_data):  \n",
    "            data_type = \"list_of_dicts\"\n",
    "    elif isinstance(raw_data, dict) and \"embeddings\" in raw_data:  \n",
    "        data_type = \"embeddings_dict\"\n",
    "    \n",
    "    if not data_type:\n",
    "        raise ValueError(\"Unrecognized Legal Sample Format\")\n",
    "    \n",
    "    return raw_data, data_type\n",
    "\n",
    "def split_and_save_legit(orig_data, data_type, split_size, output_path_1, output_path_2):\n",
    "    if data_type == \"list_of_lists\":\n",
    "        df = pd.DataFrame(orig_data)\n",
    "    elif data_type == \"list_of_dicts\":\n",
    "        df = pd.DataFrame([x[\"embedding\"] for x in orig_data])\n",
    "    elif data_type == \"embeddings_dict\":\n",
    "        df = pd.DataFrame(orig_data[\"embeddings\"])\n",
    "    \n",
    "    if split_size > len(df):\n",
    "        raise ValueError(f\"number of segments{split_size}exceeding the dataset size{len(df)}\")\n",
    "    \n",
    "    sampled = df.sample(n=split_size, random_state=42)\n",
    "    remaining = df.drop(sampled.index)\n",
    "    \n",
    "    print(f\"Segmentation Statistics\")\n",
    "    print(f\"├─ Total Number Of Original Samples: {len(df)}\")\n",
    "    print(f\"├─ Number Of Samples: {len(sampled)}\")\n",
    "    print(f\"└─ Number Of Samples Remaining: {len(remaining)}\\n\")\n",
    "    \n",
    "    def _convert_back(data_df, orig_type):\n",
    "        if orig_type == \"list_of_lists\":\n",
    "            return data_df.values.tolist()\n",
    "        elif orig_type == \"list_of_dicts\":\n",
    "            return [{\"embedding\": row.tolist()} for _, row in data_df.iterrows()]\n",
    "        elif orig_type == \"embeddings_dict\":\n",
    "            return {\"embeddings\": data_df.values.tolist()}\n",
    "    \n",
    "    def _save_data(data, path, orig_type):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    _save_data(_convert_back(sampled, data_type), output_path_1, data_type)\n",
    "    _save_data(_convert_back(remaining, data_type), output_path_2, data_type)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/legiti_embeddings_js.json\"          # the path of the original legal sample\n",
    "    output_sampled = \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/test_train/legiti_test_js.json\"     # the sample path was drawn\n",
    "    output_remaining = \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/test_train/legiti_train_js.json\" # remaining sample paths\n",
    "    split_num = 400    # spilt number\n",
    "\n",
    "    try:\n",
    "        orig_data, data_format = load_legit_data(input_path)\n",
    "        \n",
    "        split_and_save_legit(\n",
    "            orig_data = orig_data,\n",
    "            data_type = data_format,\n",
    "            split_size = split_num,\n",
    "            output_path_1 = output_sampled,\n",
    "            output_path_2 = output_remaining\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully Split And Save：\\nSamples Were Drawn -> {output_sampled}\\nRemaining Samples -> {output_remaining}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"处理失败：{str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malicious Packages\n",
    "\n",
    "CONFIG = {\n",
    "    \"input_path\": \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/malware_clusters_results_js.json\",\n",
    "    \"output_sampled\": \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/test_train/mal_test_js.json\",\n",
    "    \"output_remaining\": \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/test_train/mal_train_js.json\",\n",
    "    \"total_samples\": 400,\n",
    "    \"allow_repeats\": False\n",
    "}\n",
    "\n",
    "def main():\n",
    "    input_file = CONFIG[\"input_path\"]\n",
    "    \n",
    "    try:\n",
    "        print(f\"Reading Input File: {input_file}\")\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        exit(f\"Failed to read file: {str(e)}\")\n",
    "\n",
    "    print(f\"Cluster sampling started. Number of requested samples: {CONFIG['total_samples']}\")\n",
    "    try:\n",
    "        # Perform cluster sampling and preserve the original structure\n",
    "        sampled_data, remaining_data, stats = cluster_sampling(\n",
    "            raw_data, \n",
    "            CONFIG[\"total_samples\"],\n",
    "            CONFIG[\"allow_repeats\"]\n",
    "        )\n",
    "        \n",
    "        save_cluster_data(sampled_data, CONFIG[\"output_sampled\"])\n",
    "        save_cluster_data(remaining_data, CONFIG[\"output_remaining\"])\n",
    "        \n",
    "    except Exception as e:\n",
    "        exit(f\"Error: {str(e)}\")\n",
    "\n",
    "    # Print detailed sampling statistics\n",
    "    print_statistics(stats)\n",
    "\n",
    "def print_statistics(stats):\n",
    "    total_sampled = sum(info['sampled'] for info in stats.values())\n",
    "    total_remaining = sum(info['remaining'] for info in stats.values())\n",
    "    \n",
    "    print(\"\\n=== Cluster Sampling Statistics ===\")\n",
    "    print(f\"Total Sample Requested: {CONFIG['total_samples']}\")\n",
    "    print(f\"Actual Number of Samples Drawn: {total_sampled}\")\n",
    "    print(f\"Total Remaining Samples: {total_remaining}\\n\")\n",
    "    print(\"Per-Cluster Details:\")\n",
    "    print(f\"{'Cluster ID':<10} {'Allocated':<8} {'Original':<8} {'Sampled':<8} {'Remaining':<8} {'Status':<12}\")\n",
    "    print(\"-\" * 65)\n",
    "    for cluster_id, info in stats.items():\n",
    "        status = \"With Repeats\" if info['repeats_used'] else \"Normal\"\n",
    "        print(f\"{cluster_id:<10} {info['allocated']:<8} {info['original']:<8} \"\n",
    "              f\"{info['sampled']:<8} {info['remaining']:<8} {status:<12}\")\n",
    "\n",
    "\n",
    "def save_cluster_data(data, path):\n",
    "    \"\"\"Save cluster data (preserve original format)\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    # Auto-detect original format\n",
    "    if isinstance(data, list):\n",
    "        output = []\n",
    "        for cluster in data:\n",
    "            output.append({\n",
    "                cluster['id']: {\n",
    "                    \"size\": cluster['size'],\n",
    "                    \"samples\": cluster['samples']\n",
    "                }\n",
    "            })\n",
    "    else:\n",
    "        output = {\n",
    "            cluster['id']: {\n",
    "                \"size\": cluster['size'],\n",
    "                \"samples\": cluster['samples']\n",
    "            } for cluster in data.values()\n",
    "        }\n",
    "    \n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def cluster_sampling(raw_data, total_samples, allow_repeats):\n",
    "    \"\"\"Improved cluster sampling logic ensuring each cluster has at least one sample,\n",
    "       and remaining samples are allocated proportionally.\"\"\"\n",
    "    \n",
    "    # Normalize structure\n",
    "    if isinstance(raw_data, list):\n",
    "        processed_data = {}\n",
    "        for item in raw_data:\n",
    "            processed_data.update(item)\n",
    "    else:\n",
    "        processed_data = raw_data\n",
    "\n",
    "    # Remove invalid clusters (size <= 0)\n",
    "    valid_clusters = {\n",
    "        k: v for k, v in processed_data.items() \n",
    "        if isinstance(v.get(\"size\", 0), (int, float)) and v[\"size\"] > 0\n",
    "    }\n",
    "    if not valid_clusters:\n",
    "        raise ValueError(\"No valid clusters found (all have size <= 0)\")\n",
    "\n",
    "    # Initialize stats dicts\n",
    "    stats = {}\n",
    "    sampled_clusters = {}\n",
    "    remaining_clusters = {}\n",
    "\n",
    "    # Total cluster count\n",
    "    num_clusters = len(valid_clusters)\n",
    "    if total_samples < num_clusters:\n",
    "        raise ValueError(f\"Not enough total samples {total_samples} to assign at least one per cluster.\")\n",
    "\n",
    "    # Pre-assign 1 sample per cluster\n",
    "    base_alloc = {cluster_id: 1 for cluster_id in valid_clusters}\n",
    "    remaining_samples = total_samples - num_clusters\n",
    "\n",
    "    # Total size for proportional allocation\n",
    "    total_size = sum(details[\"size\"] for details in valid_clusters.values())\n",
    "\n",
    "    # Proportional allocation of remaining samples\n",
    "    allocations = base_alloc.copy()\n",
    "    for cluster_id, details in valid_clusters.items():\n",
    "        if total_size > 0:\n",
    "            alloc = round((details[\"size\"] / total_size) * remaining_samples)\n",
    "        else:\n",
    "            alloc = 0  # Prevent division by zero\n",
    "        allocations[cluster_id] += alloc\n",
    "\n",
    "    # Fix potential rounding discrepancy\n",
    "    allocated_sum = sum(allocations.values())\n",
    "    if allocated_sum < total_samples:\n",
    "        # Add leftover to largest cluster\n",
    "        largest_cluster_id = max(valid_clusters.items(), key=lambda x: x[1][\"size\"])[0]\n",
    "        allocations[largest_cluster_id] += (total_samples - allocated_sum)\n",
    "\n",
    "    # Sample from clusters\n",
    "    for cluster_id, details in valid_clusters.items():\n",
    "        original_samples = details[\"samples\"]\n",
    "        need = allocations.get(cluster_id, 0)\n",
    "\n",
    "        # Init cluster stats\n",
    "        cluster_stat = {\n",
    "            \"original\": len(original_samples),\n",
    "            \"allocated\": need,\n",
    "            \"sampled\": 0,\n",
    "            \"remaining\": len(original_samples),\n",
    "            \"repeats_used\": False\n",
    "        }\n",
    "\n",
    "        if need > 0:\n",
    "            if len(original_samples) < need:\n",
    "                if not allow_repeats:\n",
    "                    print(f\"Warning: Cluster {cluster_id} has insufficient samples (requested {need}, available {len(original_samples)}). Adjusted.\")\n",
    "                    need = len(original_samples)\n",
    "                    cluster_stat[\"allocated\"] = need\n",
    "                else:\n",
    "                    print(f\"Notice: Cluster {cluster_id} using repeated sampling (requested {need}, available {len(original_samples)}).\")\n",
    "                    cluster_stat[\"repeats_used\"] = True\n",
    "\n",
    "            if allow_repeats:\n",
    "                selected = random.choices(original_samples, k=need)\n",
    "                remaining_samples_list = original_samples\n",
    "            else:\n",
    "                selected = random.sample(original_samples, min(need, len(original_samples)))\n",
    "                remaining_samples_list = [s for s in original_samples if s not in selected]\n",
    "\n",
    "            cluster_stat.update({\n",
    "                \"sampled\": len(selected),\n",
    "                \"remaining\": len(remaining_samples_list)\n",
    "            })\n",
    "\n",
    "            sampled_clusters[cluster_id] = {\n",
    "                \"id\": cluster_id,\n",
    "                \"size\": len(selected),\n",
    "                \"samples\": selected\n",
    "            }\n",
    "            remaining_clusters[cluster_id] = {\n",
    "                \"id\": cluster_id,\n",
    "                \"size\": len(remaining_samples_list),\n",
    "                \"samples\": remaining_samples_list\n",
    "            }\n",
    "        else:\n",
    "            remaining_clusters[cluster_id] = {\n",
    "                \"id\": cluster_id,\n",
    "                \"size\": details[\"size\"],\n",
    "                \"samples\": details[\"samples\"]\n",
    "            }\n",
    "\n",
    "        stats[cluster_id] = cluster_stat\n",
    "\n",
    "    # Handle invalid clusters\n",
    "    for cluster_id in set(processed_data.keys()) - set(valid_clusters.keys()):\n",
    "        stats[cluster_id] = {\n",
    "            \"original\": len(processed_data[cluster_id][\"samples\"]),\n",
    "            \"allocated\": 0,\n",
    "            \"sampled\": 0,\n",
    "            \"remaining\": len(processed_data[cluster_id][\"samples\"]),\n",
    "            \"repeats_used\": False\n",
    "        }\n",
    "        remaining_clusters[cluster_id] = processed_data[cluster_id]\n",
    "\n",
    "    output_format = type(raw_data)\n",
    "    return (\n",
    "        output_format([{k:v} for k,v in sampled_clusters.items()]) if isinstance(raw_data, list) else sampled_clusters,\n",
    "        output_format([{k:v} for k,v in remaining_clusters.items()]) if isinstance(raw_data, list) else remaining_clusters,\n",
    "        stats\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unclustered malware samples\n",
    "def load_legit_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    data_type = None\n",
    "    if isinstance(raw_data, list):\n",
    "        if all(isinstance(x, list) for x in raw_data):  \n",
    "            data_type = \"list_of_lists\"\n",
    "        elif all(isinstance(x, dict) for x in raw_data):  \n",
    "            data_type = \"list_of_dicts\"\n",
    "    elif isinstance(raw_data, dict) and \"embeddings\" in raw_data:  \n",
    "        data_type = \"embeddings_dict\"\n",
    "    \n",
    "    if not data_type:\n",
    "        raise ValueError(\"Unrecognized sample format\")\n",
    "    \n",
    "    return raw_data, data_type\n",
    "\n",
    "def split_and_save_legit(orig_data, data_type, split_size, output_path_1, output_path_2):\n",
    "    \"\"\"Randomly split and save dataset (preserve original format)\"\"\"\n",
    "    # Convert to DataFrame for processing\n",
    "    if data_type == \"list_of_lists\":\n",
    "        df = pd.DataFrame(orig_data)\n",
    "    elif data_type == \"list_of_dicts\":\n",
    "        df = pd.DataFrame([x[\"embedding\"] for x in orig_data])\n",
    "    elif data_type == \"embeddings_dict\":\n",
    "        df = pd.DataFrame(orig_data[\"embeddings\"])\n",
    "    \n",
    "    # Random sampling\n",
    "    if split_size > len(df):\n",
    "        raise ValueError(f\"Split size {split_size} exceeds dataset size {len(df)}\")\n",
    "    \n",
    "    sampled = df.sample(n=split_size, random_state=42)\n",
    "    remaining = df.drop(sampled.index)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Split Summary:\")\n",
    "    print(f\"├─ Total original samples: {len(df)}\")\n",
    "    print(f\"├─ Number of sampled items: {len(sampled)}\")\n",
    "    print(f\"└─ Number of remaining items: {len(remaining)}\\n\")\n",
    "    \n",
    "    # Convert back to original format\n",
    "    def _convert_back(data_df, orig_type):\n",
    "        if orig_type == \"list_of_lists\":\n",
    "            return data_df.values.tolist()\n",
    "        elif orig_type == \"list_of_dicts\":\n",
    "            return [{\"embedding\": row.tolist()} for _, row in data_df.iterrows()]\n",
    "        elif orig_type == \"embeddings_dict\":\n",
    "            return {\"embeddings\": data_df.values.tolist()}\n",
    "    \n",
    "    def _save_data(data, path, orig_type):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    _save_data(_convert_back(sampled, data_type), output_path_1, data_type)\n",
    "    _save_data(_convert_back(remaining, data_type), output_path_2, data_type)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/mal_embeddings_js_no_cluster.json\"          # original unclustered sample path\n",
    "    output_sampled = \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/test_train/nocluster_mal_sample_js.json\"     # sampled output path \n",
    "    output_remaining = \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/test_train/nocluster_mal_remain_js.json\"  # remaining output path\n",
    "    split_num = 100                        \n",
    "\n",
    "    try:\n",
    "        orig_data, data_format = load_legit_data(input_path)\n",
    "        \n",
    "        split_and_save_legit(\n",
    "            orig_data = orig_data,\n",
    "            data_type = data_format,\n",
    "            split_size = split_num,\n",
    "            output_path_1 = output_sampled,\n",
    "            output_path_2 = output_remaining\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully split and saved:\\nSampled -> {output_sampled}\\nRemaining -> {output_remaining}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Processing failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Test Set\n",
    "CONFIG = {\n",
    "    \"legit_test_path\": \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/test_train/legiti_test_js.json\",  # Path to legitimate test samples\n",
    "    \"malware_test_paths\": [  # Multiple paths to malware samples (list supported)\n",
    "        \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/test_train/mal_test_js.json\",\n",
    "        \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/test_train/nocluster_mal_sample_js.json\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def transform_to_feature_format(df):\n",
    "    if 'label' not in df.columns:\n",
    "        raise ValueError(\"Missing 'label' column in DataFrame\")\n",
    "    \n",
    "    feature_columns = [col for col in df.columns if col != 'label']\n",
    "    df[feature_columns] = df[feature_columns].astype(float)\n",
    "    df['feature'] = df[feature_columns].apply(lambda x: x.tolist(), axis=1)\n",
    "    \n",
    "    return df[['feature', 'label']].copy()\n",
    "\n",
    "def load_labeled_data(json_paths, label):\n",
    "    \"\"\"Load data from multiple paths (supports single path or list of paths)\"\"\"\n",
    "    if not isinstance(json_paths, list):\n",
    "        json_paths = [json_paths]\n",
    "    \n",
    "    all_dfs = []\n",
    "    for path in json_paths:\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Handle dictionary with 'embeddings' key\n",
    "        if isinstance(data, dict) and \"embeddings\" in data:\n",
    "            all_dfs.append(pd.DataFrame(data[\"embeddings\"]))\n",
    "            continue\n",
    "        \n",
    "        # Handle clustered dict structure\n",
    "        if isinstance(data, dict):\n",
    "            samples = []\n",
    "            for cluster in data.values():\n",
    "                if \"samples\" in cluster:\n",
    "                    cluster_samples = cluster[\"samples\"]\n",
    "                    if cluster_samples and isinstance(cluster_samples[0], dict):\n",
    "                        samples.extend([s[\"embedding\"] for s in cluster_samples])\n",
    "                    else:\n",
    "                        samples.extend(cluster_samples)\n",
    "            all_dfs.append(pd.DataFrame(samples))\n",
    "            continue\n",
    "        \n",
    "        # Handle flat list structure\n",
    "        if isinstance(data, list):\n",
    "            if all(isinstance(x, list) for x in data):\n",
    "                all_dfs.append(pd.DataFrame(data))\n",
    "            elif all(isinstance(x, dict) for x in data):\n",
    "                all_dfs.append(pd.DataFrame([x[\"embedding\"] for x in data]))\n",
    "    \n",
    "    if not all_dfs:\n",
    "        raise ValueError(f\"Unrecognized format. Paths: {json_paths}\")\n",
    "    \n",
    "    return pd.concat(all_dfs).assign(label=label)\n",
    "\n",
    "def create_test_dataset(legit_path, malware_paths):\n",
    "    \"\"\"Create test dataset\n",
    "    :param legit_path: Path to legitimate samples\n",
    "    :param malware_paths: List of malware sample paths\n",
    "    \"\"\"\n",
    "    # Load legitimate samples (label 0)\n",
    "    print(f\"[1/3] Loading legitimate samples: {legit_path}\")\n",
    "    legit_df = load_labeled_data(legit_path, 0)\n",
    "    \n",
    "    # Load malware samples (label 1)\n",
    "    print(f\"[2/3] Loading malware samples: {len(malware_paths)} files\")\n",
    "    malware_df = load_labeled_data(malware_paths, 1)\n",
    "    \n",
    "    # Merge datasets\n",
    "    print(\"[3/3] Merging datasets...\")\n",
    "    combined = pd.concat([legit_df, malware_df])\n",
    "    processed = transform_to_feature_format(combined)\n",
    "    \n",
    "    return processed.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "test_set = create_test_dataset(\n",
    "        CONFIG[\"legit_test_path\"],\n",
    "        CONFIG[\"malware_test_paths\"]\n",
    "    )\n",
    "\n",
    "# Print validation info\n",
    "print(\"\\n=== Test Set Info ===\")\n",
    "print(f\"Total Samples: {len(test_set)}\")\n",
    "print(\"Label Distribution:\")\n",
    "print(test_set['label'].value_counts())\n",
    "print(\"\\nFeature Dimension:\", len(test_set.iloc[0]['feature']))\n",
    "print(\"First 3 Samples:\")\n",
    "print(test_set.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling to generate training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Sampling\n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    \"input_path\": \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/test_train/nocluster_mal_remain_js.json\",  \n",
    "    \"total_samples\": 400,  # Number of training samples\n",
    "    \"allow_repeats\": True  # Whether to allow repeated sampling\n",
    "}\n",
    "# ====================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    input_file = CONFIG[\"input_path\"]\n",
    "    sampling_stats = None  # Explicitly initialize stats variable\n",
    "    \n",
    "    try:\n",
    "        print(f\"Reading input file: {input_file}\")\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)\n",
    "            \n",
    "            # Validate format\n",
    "            if \"embeddings\" not in raw_data or not isinstance(raw_data[\"embeddings\"], list):\n",
    "                raise ValueError(\"Invalid input file format. It must contain a list field 'embeddings'.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        exit(f\"Failed to read file: {str(e)}\")\n",
    "\n",
    "    print(f\"Starting global random sampling, target size: {CONFIG['total_samples']}\")\n",
    "    try:\n",
    "        # Convert embeddings to sample format\n",
    "        all_samples = [{\"embedding\": emb} for emb in raw_data[\"embeddings\"]]\n",
    "        \n",
    "        # Perform sampling\n",
    "        train_set, sampling_stats = global_random_sampling(\n",
    "            all_samples,\n",
    "            CONFIG[\"total_samples\"],\n",
    "            CONFIG[\"allow_repeats\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        exit(f\"Error during sampling: {str(e)}\")\n",
    "\n",
    "    # Print statistics if available\n",
    "    if sampling_stats:\n",
    "        print_sampling_stats(sampling_stats)\n",
    "    else:\n",
    "        print(\"Warning: No sampling stats generated\")\n",
    "\n",
    "    return train_set\n",
    "\n",
    "def global_random_sampling(all_samples, total_samples, allow_repeats):\n",
    "    \"\"\"Enhanced global random sampling logic\"\"\"\n",
    "    if len(all_samples) == 0:\n",
    "        raise ValueError(\"No available samples in input data.\")\n",
    "\n",
    "    # Initialize stats\n",
    "    sampling_stats = {\n",
    "        \"total_samples\": len(all_samples),\n",
    "        \"requested_samples\": total_samples,\n",
    "        \"allow_repeats\": allow_repeats,\n",
    "        \"actual_sampled\": 0,\n",
    "        \"repeats_used\": False\n",
    "    }\n",
    "\n",
    "    # Handle insufficient sample case\n",
    "    if not allow_repeats and len(all_samples) < total_samples:\n",
    "        print(f\"Warning: Not enough available samples ({len(all_samples)}), adjusted sample size automatically.\")\n",
    "        total_samples = len(all_samples)\n",
    "        sampling_stats[\"requested_samples\"] = total_samples\n",
    "\n",
    "    # Perform sampling\n",
    "    try:\n",
    "        selected = random.choices(all_samples, k=total_samples) if allow_repeats else random.sample(all_samples, total_samples)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Sampling failed: {str(e)}\")\n",
    "\n",
    "    # Update stats\n",
    "    sampling_stats[\"actual_sampled\"] = len(selected)\n",
    "    sampling_stats[\"repeats_used\"] = allow_repeats and (len(selected) > len(all_samples))\n",
    "\n",
    "    # Extract embedding list\n",
    "    train_set = [s[\"embedding\"] for s in selected if \"embedding\" in s]\n",
    "    return train_set, sampling_stats\n",
    "\n",
    "def print_sampling_stats(stats):\n",
    "    \"\"\"Enhanced stats printing\"\"\"\n",
    "    print(\"\\n=== Sampling Stats ===\")\n",
    "    print(f\"Total available samples: {stats['total_samples']}\")\n",
    "    print(f\"Requested sample count: {stats['requested_samples']}\")\n",
    "    print(f\"Actual samples drawn: {stats['actual_sampled']}\")\n",
    "    print(f\"Allow duplicates: {'Yes' if stats['allow_repeats'] else 'No'}\")\n",
    "    print(f\"Duplicates used: {'Yes' if stats['repeats_used'] else 'No'}\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "# Run main program\n",
    "malware_set_1 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster-based Sampling\n",
    "\n",
    "CONFIG = {\n",
    "    \"input_path\": \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/test_train/mal_train_js.json\",  # Input file path\n",
    "    \"total_samples\": 400,  # Number of training samples\n",
    "    \"allow_repeats\": False  # Allow repeated sampling (when samples are insufficient)\n",
    "}\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    input_file = CONFIG[\"input_path\"]\n",
    "    \n",
    "    try:\n",
    "        print(f\"Reading input file: {input_file}\")\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        exit(f\"Failed to read file: {str(e)}\")\n",
    "\n",
    "    print(f\"Starting sampling, target sample count: {CONFIG['total_samples']}\")\n",
    "    try:\n",
    "        # Perform cluster-based proportional sampling\n",
    "        train_set, sampling_stats = cluster_sampling(\n",
    "            raw_data, \n",
    "            CONFIG[\"total_samples\"],\n",
    "            CONFIG[\"allow_repeats\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        exit(f\"Sampling error: {str(e)}\")\n",
    "\n",
    "    # Print statistics\n",
    "    print_sampling_stats(sampling_stats)\n",
    "\n",
    "    # Return the training set for further use\n",
    "    return train_set\n",
    "\n",
    "def print_sampling_stats(stats):\n",
    "    \"\"\"Print detailed sampling statistics\"\"\"\n",
    "    print(\"\\n=== Sampling Statistics ===\")\n",
    "    print(f\"{'Cluster':<8} {'Allocated':<8} {'Original':<8} {'Sampled':<8} {'Status':<10} {'Note'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for cluster_id, info in stats.items():\n",
    "        status = \"OK\" if info[\"sampled\"] == info[\"allocated\"] else \"Insufficient\"\n",
    "        note = \"With repeats\" if info[\"repeats_used\"] else \"\"\n",
    "        \n",
    "        print(f\"{cluster_id:<8} {info['allocated']:<8} {info['original_samples']:<8} \"\n",
    "              f\"{info['sampled']:<8} {status:<10} {note}\")\n",
    "\n",
    "def cluster_sampling(raw_data, total_samples, allow_repeats):\n",
    "    \"\"\"Core sampling logic\"\"\"\n",
    "    # Data preprocessing\n",
    "    if isinstance(raw_data, list):\n",
    "        processed_data = {}\n",
    "        for item in raw_data:\n",
    "            processed_data.update(item)\n",
    "    else:\n",
    "        processed_data = raw_data\n",
    "\n",
    "    # Validate data structure\n",
    "    if not isinstance(processed_data, dict):\n",
    "        raise ValueError(\"Input data must be a dict or a list of dicts\")\n",
    "\n",
    "    # Build cluster info\n",
    "    clusters = []\n",
    "    for cluster_id, details in processed_data.items():\n",
    "        clusters.append({\n",
    "            \"id\": str(cluster_id),\n",
    "            \"size\": details.get(\"size\", 0),\n",
    "            \"samples\": details.get(\"samples\", [])\n",
    "        })\n",
    "\n",
    "    # Perform proportional sampling\n",
    "    return proportional_sampling(clusters, total_samples, allow_repeats)\n",
    "\n",
    "def proportional_sampling(clusters, total_samples, allow_repeats):\n",
    "    \"\"\"Sampling logic with proportional allocation\"\"\"\n",
    "    # Initialize stats\n",
    "    sampling_stats = {}\n",
    "    \n",
    "    # Compute total size\n",
    "    total_size = sum(c[\"size\"] for c in clusters)\n",
    "    if total_size == 0:\n",
    "        raise ValueError(\"Total size across all clusters cannot be zero\")\n",
    "\n",
    "    # Allocate samples\n",
    "    allocations = {}\n",
    "    remaining = total_samples\n",
    "    for c in clusters:\n",
    "        alloc = round(c[\"size\"] / total_size * total_samples)\n",
    "        allocations[c[\"id\"]] = min(alloc, remaining)\n",
    "        remaining -= alloc\n",
    "\n",
    "    # Handle remaining quota\n",
    "    if remaining > 0:\n",
    "        largest_cluster = max(clusters, key=lambda x: x[\"size\"])\n",
    "        allocations[largest_cluster[\"id\"]] += remaining\n",
    "\n",
    "    # Perform sampling\n",
    "    train_set = []\n",
    "    for c in clusters:\n",
    "        cluster_id = c[\"id\"]\n",
    "        need = allocations.get(cluster_id, 0)\n",
    "        samples = c[\"samples\"]\n",
    "        original_size = len(samples)\n",
    "        \n",
    "        # Initialize cluster stats\n",
    "        stats = {\n",
    "            \"allocated\": need,\n",
    "            \"original_samples\": original_size,\n",
    "            \"sampled\": 0,\n",
    "            \"repeats_used\": False\n",
    "        }\n",
    "\n",
    "        # Handle insufficient samples\n",
    "        if original_size < need:\n",
    "            if not allow_repeats:\n",
    "                print(f\"Warning: Cluster {cluster_id} has insufficient samples (requested {need}, available {original_size})\")\n",
    "                need = original_size\n",
    "                stats[\"allocated\"] = need\n",
    "            else:\n",
    "                print(f\"Note: Cluster {cluster_id} allows repeated sampling (requested {need}, available {original_size})\")\n",
    "                stats[\"repeats_used\"] = True\n",
    "\n",
    "        # Perform sampling\n",
    "        if need > 0:\n",
    "            if allow_repeats:\n",
    "                selected = random.choices(samples, k=need)\n",
    "            else:\n",
    "                selected = random.sample(samples, min(need, original_size))\n",
    "            \n",
    "            # Record stats\n",
    "            stats[\"sampled\"] = len(selected)\n",
    "            sampling_stats[cluster_id] = stats\n",
    "            \n",
    "            # Extract embeddings\n",
    "            train_set.extend([s[\"embedding\"] for s in selected if \"embedding\" in s])\n",
    "\n",
    "    return train_set[:total_samples], sampling_stats\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "malware_set_2 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Training Sets\n",
    "def load_labeled_data(json_path, label):\n",
    "    \"\"\"Universal data loading function (supports multiple formats)\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Format 1: list of lists [[0.1,...],...]\n",
    "    if isinstance(data, list) and all(isinstance(x, list) for x in data):\n",
    "        return pd.DataFrame(data).assign(label=label)\n",
    "    \n",
    "    # Format 2: list of dicts [{\"embedding\": [...]}, ...]\n",
    "    if isinstance(data, list) and all(\"embedding\" in x for x in data):\n",
    "        return pd.DataFrame([x[\"embedding\"] for x in data]).assign(label=label)\n",
    "    \n",
    "    # Format 3: dict with \"embeddings\" key {\"embeddings\": [[...], ...]}\n",
    "    if isinstance(data, dict) and \"embeddings\" in data:\n",
    "        return pd.DataFrame(data[\"embeddings\"]).assign(label=label)\n",
    "    \n",
    "    raise ValueError(f\"Unsupported JSON format: {json_path}\")\n",
    "\n",
    "def transform_to_feature_format(df):\n",
    "    \"\"\"Convert DataFrame into format with feature list\"\"\"\n",
    "    if 'label' not in df.columns:\n",
    "        raise ValueError(\"Missing 'label' column in DataFrame\")\n",
    "    \n",
    "    feature_columns = [col for col in df.columns if col != 'label']\n",
    "    df[feature_columns] = df[feature_columns].astype(float)\n",
    "    df['feature'] = df[feature_columns].apply(lambda x: x.tolist(), axis=1)\n",
    "    \n",
    "    return df[['feature', 'label']].copy()\n",
    "\n",
    "def create_processed_dataset(legit_df, malware_data, samples_per_class):\n",
    "    \"\"\"Create dataset ready for machine learning\"\"\"\n",
    "    if isinstance(malware_data, list):\n",
    "        malware_df = pd.DataFrame(malware_data).assign(label=1)\n",
    "    elif isinstance(malware_data, dict):\n",
    "        malware_df = pd.DataFrame(malware_data[\"embeddings\"]).assign(label=1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported malware data format\")\n",
    "    \n",
    "    sampled_legit = legit_df.sample(\n",
    "        n=samples_per_class,\n",
    "        replace=len(legit_df) < samples_per_class,\n",
    "        random_state=42\n",
    "    )\n",
    "    sampled_malware = malware_df.sample(\n",
    "        n=samples_per_class,\n",
    "        replace=len(malware_df) < samples_per_class,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    combined = pd.concat([sampled_legit, sampled_malware])\n",
    "    processed_df = transform_to_feature_format(combined)\n",
    "    final_df = processed_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    legit_path = \"/Users/zhouxiaoyan/Documents/GIthub/OSS-MPAW/ML/test_train/legiti_train_js.json\"  # Legitimate samples path\n",
    "    samples_per_class = 400  # Number of samples per class\n",
    "\n",
    "    legit_df = load_labeled_data(legit_path, label=0)\n",
    "\n",
    "    # Create two training sets\n",
    "    train_set_1 = create_processed_dataset(legit_df, malware_set_1, samples_per_class)  # Random sampling\n",
    "    train_set_2 = create_processed_dataset(legit_df, malware_set_2, samples_per_class)  # Cluster-based sampling\n",
    "    return train_set_1, train_set_2\n",
    "\n",
    "train_set_1, train_set_2 = main()\n",
    "print(f\"Training set 1 size: {len(train_set_1)}\")\n",
    "print(f\"Training set 2 size: {len(train_set_2)}\")\n",
    "print(train_set_1.head())\n",
    "print(train_set_2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endan = []\n",
    "endan2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_expanded = pd.DataFrame(X_train['feature'].tolist())\n",
    "    X_train_expanded.fillna(0, inplace=True)  \n",
    "    X_train_scaled = scaler.fit_transform(X_train_expanded)  \n",
    "\n",
    "    X_test_expanded = pd.DataFrame(X_test['feature'].tolist())\n",
    "    X_test_expanded.fillna(0, inplace=True)  \n",
    "    X_test_scaled = scaler.transform(X_test_expanded)  \n",
    "\n",
    "    return X_train_scaled, X_test_scaled, X_train['label'], X_test['label']\n",
    "\n",
    "def train_and_predict(model, X_train, X_test, y_train, param_grid):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    X_test_expanded = X_test.astype(float)\n",
    "    y_pred = best_model.predict(X_test_expanded)\n",
    "\n",
    "    return y_pred, best_model, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "def traditional_machine(train_set_1, test_set):\n",
    "    #  use train_set_1 and test_set\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(train_set_1, test_set)\n",
    "\n",
    "    models_and_params = {\n",
    "        # LogisticRegression(): {'C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "        # DecisionTreeClassifier(): {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "        # RandomForestClassifier(): {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "        # SVC(): {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "        # KNeighborsClassifier(): {'n_neighbors': [3, 5, 7]},\n",
    "        # MLPClassifier(): {'hidden_layer_sizes': [(50,), (100,), (50, 50)]}\n",
    "        LogisticRegression(): {'C': [0.1]},\n",
    "        DecisionTreeClassifier(): {'max_depth': [30], 'min_samples_split': [5]},\n",
    "        RandomForestClassifier(): {'n_estimators': [200], 'max_depth': [None], 'min_samples_split': [5]},\n",
    "        KNeighborsClassifier(): {'n_neighbors': [5]},\n",
    "        MLPClassifier(): {'hidden_layer_sizes': [(50,50)]}\n",
    "    }\n",
    "\n",
    "    for model, params in models_and_params.items():\n",
    "        model_name = model.__class__.__name__\n",
    "        print(f\"Tuning {model_name} parameters and performing cross-validation evaluation...\")\n",
    "\n",
    "        y_pred, best_model, best_params, best_score = train_and_predict(model, X_train_scaled, X_test_scaled, y_train, params)\n",
    "\n",
    "        # Output best model and parameters\n",
    "        print(f\"Best model: {best_model}\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best cross-validation accuracy: {best_score:.2f}\")\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"{model_name} test accuracy: {accuracy:.10f}\")\n",
    "        print(f\"{model_name} Precision: {precision:.10f}\")\n",
    "        print(f\"{model_name} Recall: {recall:.10f}\")\n",
    "        print(f\"{model_name} F1-Score: {f1:.10f}\")\n",
    "        endan.append([model_name, best_params, accuracy, precision, recall, f1])\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"{model_name} Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "# Baseline traditional machine train\n",
    "traditional_machine(train_set_1, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_expanded = pd.DataFrame(X_train['feature'].tolist())\n",
    "    X_train_expanded.fillna(0, inplace=True)  \n",
    "    X_train_scaled = scaler.fit_transform(X_train_expanded)  \n",
    "\n",
    "    X_test_expanded = pd.DataFrame(X_test['feature'].tolist())\n",
    "    X_test_expanded.fillna(0, inplace=True)  \n",
    "    X_test_scaled = scaler.transform(X_test_expanded)  \n",
    "\n",
    "    return X_train_scaled, X_test_scaled, X_train['label'], X_test['label']\n",
    "\n",
    "def train_and_predict(model, X_train, X_test, y_train, param_grid):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    X_test_expanded = X_test.astype(float)\n",
    "    y_pred = best_model.predict(X_test_expanded)\n",
    "\n",
    "    return y_pred, best_model, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "def traditional_machine(train_set_2, test_set):\n",
    "    # use train_set_2 和 test_set\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(train_set_2, test_set)\n",
    "\n",
    "    models_and_params = {\n",
    "        # LogisticRegression(): {'C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "        # DecisionTreeClassifier(): {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "        # RandomForestClassifier(): {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "        # KNeighborsClassifier(): {'n_neighbors': [3, 5, 7]},\n",
    "        # MLPClassifier(): {'hidden_layer_sizes': [(50,), (100,), (50, 50)]}\n",
    "        LogisticRegression(): {'C': [0.1]},\n",
    "        DecisionTreeClassifier(): {'max_depth': [30], 'min_samples_split': [5]},\n",
    "        RandomForestClassifier(): {'n_estimators': [200], 'max_depth': [None], 'min_samples_split': [5]},\n",
    "        KNeighborsClassifier(): {'n_neighbors': [5]},\n",
    "        MLPClassifier(): {'hidden_layer_sizes': [(50,50)]}\n",
    "\n",
    "    }\n",
    "\n",
    "    for model, params in models_and_params.items():\n",
    "        model_name = model.__class__.__name__\n",
    "        print(f\"Tuning {model_name} parameters and performing cross-validation evaluation...\")\n",
    "\n",
    "        y_pred, best_model, best_params, best_score = train_and_predict(model, X_train_scaled, X_test_scaled, y_train, params)\n",
    "\n",
    "        # Output best model and parameters\n",
    "        print(f\"Best model: {best_model}\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best cross-validation accuracy: {best_score:.2f}\")\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"{model_name} test accuracy: {accuracy:.10f}\")\n",
    "        print(f\"{model_name} Precision: {precision:.10f}\")\n",
    "        print(f\"{model_name} Recall: {recall:.10f}\")\n",
    "        print(f\"{model_name} F1-Score: {f1:.10f}\")\n",
    "        endan2.append([model_name, best_params, accuracy, precision, recall, f1])\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"{model_name} Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "\n",
    "#  cluster machine train\n",
    "traditional_machine(train_set_2, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_set, test_set):\n",
    "    # Preprocess the training and testing datasets\n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(train_set['feature'].tolist()).astype(float)\n",
    "    X_train.fillna(0, inplace=True)  # Fill NaN values\n",
    "    X_train_scaled = scaler.fit_transform(X_train)  # Standardize training set\n",
    "\n",
    "    # Extract and preprocess test set features\n",
    "    X_test = pd.DataFrame(test_set['feature'].tolist()).astype(float)\n",
    "    X_test.fillna(0, inplace=True)  # Fill NaN values\n",
    "    X_test_scaled = scaler.transform(X_test)  # Standardize test set\n",
    "\n",
    "    # Get labels\n",
    "    y_train = train_set['label'].values\n",
    "    y_test = test_set['label'].values\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "def train_cnn(train_set, test_set):\n",
    "    # Data preprocessing\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(train_set, test_set)\n",
    "\n",
    "    # Reshape data for CNN\n",
    "    X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Convert y labels to categorical format (for binary classification)\n",
    "    y_train_categorical = to_categorical(y_train)\n",
    "    y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "    # CNN hyperparameter grid search\n",
    "    param_grid = {\n",
    "        'epochs': [20],\n",
    "        'batch_size': [32],\n",
    "        'filters': [32],\n",
    "        'kernel_size': [5]\n",
    "    }\n",
    "\n",
    "    best_params = None\n",
    "    best_performance = 0.0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Hyperparameter tuning via grid search\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(\"\\nCurrent Parameters:\", params)\n",
    "\n",
    "        # Build CNN model\n",
    "        cnn_model = Sequential()\n",
    "        cnn_model.add(Conv1D(filters=params['filters'], kernel_size=params['kernel_size'], activation='relu', input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2])))\n",
    "        cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "        cnn_model.add(Flatten())\n",
    "        cnn_model.add(Dense(1, activation='sigmoid'))  # Binary classification using sigmoid activation\n",
    "\n",
    "        cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        cnn_model.fit(X_train_cnn, y_train, epochs=params['epochs'], batch_size=params['batch_size'], validation_split=0.1)\n",
    "\n",
    "        # Predict on test set\n",
    "        cnn_predictions = cnn_model.predict(X_test_cnn)\n",
    "        cnn_predictions = (cnn_predictions > 0.5).astype(int)  # Convert to binary labels\n",
    "\n",
    "        # Evaluate performance\n",
    "        cnn_accuracy = accuracy_score(y_test, cnn_predictions)\n",
    "        cnn_precision = precision_score(y_test, cnn_predictions)\n",
    "        cnn_recall = recall_score(y_test, cnn_predictions)\n",
    "        cnn_f1 = f1_score(y_test, cnn_predictions)\n",
    "\n",
    "        print(\"CNN Accuracy:\", cnn_accuracy)\n",
    "        print(\"CNN Precision:\", cnn_precision)\n",
    "        print(\"CNN Recall:\", cnn_recall)\n",
    "        print(\"CNN F1-Score:\", cnn_f1)\n",
    "\n",
    "        cnn_cm = confusion_matrix(y_test, cnn_predictions)\n",
    "        print(\"CNN Confusion Matrix:\\n\", cnn_cm)\n",
    "\n",
    "        # Update best performance\n",
    "        if cnn_accuracy > best_performance:\n",
    "            best_performance = cnn_accuracy\n",
    "            best_params = params\n",
    "            best_precision = cnn_precision\n",
    "            best_recall = cnn_recall\n",
    "            best_f1 = cnn_f1\n",
    "\n",
    "    # Output best results\n",
    "    print(\"\\n=== Best CNN Model Results ===\")\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Performance (Accuracy):\", best_performance)\n",
    "    print(\"CNN Recall:\", best_recall)\n",
    "    print(\"CNN Precision:\", best_precision)\n",
    "    print(\"CNN F1-Score:\", best_f1)\n",
    "\n",
    "    # Record best result\n",
    "    endan.append(['CNN', best_params, best_performance, best_precision, best_recall, best_f1])\n",
    "\n",
    "# baseline\n",
    "train_cnn(train_set_1, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_set, test_set):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Extract and preprocess training set features\n",
    "    X_train = pd.DataFrame(train_set['feature'].tolist()).astype(float)\n",
    "    X_train.fillna(0, inplace=True)  # Fill NaN values\n",
    "    X_train_scaled = scaler.fit_transform(X_train)  # Standardize training set\n",
    "\n",
    "    # Extract and preprocess test set features\n",
    "    X_test = pd.DataFrame(test_set['feature'].tolist()).astype(float)\n",
    "    X_test.fillna(0, inplace=True)  # Fill NaN values\n",
    "    X_test_scaled = scaler.transform(X_test)  # Standardize test set\n",
    "\n",
    "    # Get labels\n",
    "    y_train = train_set['label'].values\n",
    "    y_test = test_set['label'].values\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "def train_cnn(train_set, test_set):\n",
    "    # Data preprocessing\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(train_set, test_set)\n",
    "\n",
    "    # Reshape data for CNN\n",
    "    X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Convert y labels to categorical format (if binary classification)\n",
    "    y_train_categorical = to_categorical(y_train)\n",
    "    y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "    # CNN hyperparameter grid search\n",
    "    param_grid = {\n",
    "        'epochs': [20],\n",
    "        'batch_size': [32],\n",
    "        'filters': [32],\n",
    "        'kernel_size': [5]\n",
    "    }\n",
    "\n",
    "    best_params = None\n",
    "    best_performance = 0.0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Perform hyperparameter tuning using grid search\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(\"\\nCurrent Parameters:\", params)\n",
    "\n",
    "        # Build CNN model\n",
    "        cnn_model = Sequential()\n",
    "        cnn_model.add(Conv1D(filters=params['filters'], kernel_size=params['kernel_size'], activation='relu', input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2])))\n",
    "        cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "        cnn_model.add(Flatten())\n",
    "        cnn_model.add(Dense(1, activation='sigmoid'))  # Binary classification with sigmoid activation\n",
    "\n",
    "        cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        cnn_model.fit(X_train_cnn, y_train, epochs=params['epochs'], batch_size=params['batch_size'], validation_split=0.1)\n",
    "\n",
    "        # Predict on test set\n",
    "        cnn_predictions = cnn_model.predict(X_test_cnn)\n",
    "        cnn_predictions = (cnn_predictions > 0.5).astype(int)  # Convert to binary labels\n",
    "\n",
    "        # Calculate performance metrics\n",
    "        cnn_accuracy = accuracy_score(y_test, cnn_predictions)\n",
    "        cnn_precision = precision_score(y_test, cnn_predictions)\n",
    "        cnn_recall = recall_score(y_test, cnn_predictions)\n",
    "        cnn_f1 = f1_score(y_test, cnn_predictions)\n",
    "\n",
    "        print(\"CNN Accuracy:\", cnn_accuracy)\n",
    "        print(\"CNN Precision:\", cnn_precision)\n",
    "        print(\"CNN Recall:\", cnn_recall)\n",
    "        print(\"CNN F1-Score:\", cnn_f1)\n",
    "\n",
    "        cnn_cm = confusion_matrix(y_test, cnn_predictions)\n",
    "        print(\"CNN Confusion Matrix:\\n\", cnn_cm)\n",
    "\n",
    "        # Update best performance\n",
    "        if cnn_accuracy > best_performance:\n",
    "            best_performance = cnn_accuracy\n",
    "            best_params = params\n",
    "            best_precision = cnn_precision\n",
    "            best_recall = cnn_recall\n",
    "            best_f1 = cnn_f1\n",
    "\n",
    "    # Output best parameters and results\n",
    "    print(\"\\n=== Best CNN Model Results ===\")\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Performance (Accuracy):\", best_performance)\n",
    "    print(\"CNN Recall:\", best_recall)\n",
    "    print(\"CNN Precision:\", best_precision)\n",
    "    print(\"CNN F1-Score:\", best_f1)\n",
    "\n",
    "    # Record best result\n",
    "    endan2.append(['CNN', best_params, best_performance, best_precision, best_recall, best_f1])\n",
    "\n",
    "# Cluster-based training\n",
    "train_cnn(train_set_2, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def merge_results(baseline, cluster):\n",
    "    merged = []\n",
    "    headers = [\n",
    "        \"Model\",\n",
    "        \"Baseline Acc\", \"Cluster Acc\",\n",
    "        \"Baseline Prec\", \"Cluster Prec\",\n",
    "        \"Baseline Recall\", \"Cluster Recall\",\n",
    "        \"Baseline F1\", \"Cluster F1\"\n",
    "    ]\n",
    "    \n",
    "    for b, c in zip(baseline, cluster):\n",
    "        merged.append([\n",
    "            b[0],\n",
    "            f\"{b[2]:.4f}\", f\"{c[2]:.4f}\",\n",
    "            f\"{b[3]:.4f}\", f\"{c[3]:.4f}\",\n",
    "            f\"{b[4]:.4f}\", f\"{c[4]:.4f}\",\n",
    "            f\"{b[5]:.4f}\", f\"{c[5]:.4f}\"\n",
    "        ])\n",
    "    return tabulate(merged, headers=headers, tablefmt=\"fancy_grid\")\n",
    "\n",
    "print(merge_results(endan, endan2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
